{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNDZn6SR0vYkBMm/c4UNE3J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Srishti0811/Hibernate/blob/main/Week_1_Deep%20Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep Learning\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Prerequisites**\n",
        "\n",
        "The requirements for deep learning can vary depending on the specific task and complexity of the problem. However, some of the general requirements include:\n",
        "1. Maths: high school math is sufficient.\n",
        "2. Data:  Deep learning models often require large amounts of data to effectively learn patterns and make accurate predictions.\n",
        "3. Expensive Computers: Training deep learning models can be computationally intensive. While powerful hardware accelerators like GPUs or TPUs are commonly used, cloud services can be employed to manage costs.\n",
        "\n",
        "\n",
        "Deep learning is a computer methodology aimed at extracting and transforming data. It employs multiple layers of neural networks, with each layer refining inputs from preceding layers. These layers undergo training through algorithms that minimize errors and enhance accuracy, allowing the network to learn and execute specific tasks.\n",
        "\n",
        "Different areas which use deep learning include Natural language processing (NLP), Computer Vision, Image generation etc."
      ],
      "metadata": {
        "id": "s1reLgKQ2XWA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**History Of Neural Networks**\n",
        "\n",
        "Neural networks have undergone a captivating historical journey, marked by pivotal advancements and fluctuations in interest. A concise overview of key milestones in the evolution of neural networks is presented below:\n",
        "\n",
        "- McCulloch-Pitts Model (1943): Foundational work by Warren McCulloch and Walter Pitts established the conceptual framework for artificial neurons, laying the groundwork for subsequent developments in neural networks.\n",
        "\n",
        "- Rosenblatt's Perceptron (1957): Frank Rosenblatt's creation of the perceptron, an early neural network, demonstrated its capability to learn and classify patterns, representing an early practical application of neural network concepts.\n",
        "\n",
        "- Perceptron Limitations (1969): Minsky and Papert's book \"Perceptrons\" highlighted the constraints of single-layer perceptrons, contributing to a decline in neural network interest during the 1970s and 1980s. Showed that multiple layers could address these limitations, but this insight was not widely recognized.\n",
        "\n",
        "- Pivotal Work: Parallel Distributed Processing (PDP): Released in 1986 by MIT Press, authored by David Rumelhart, James McClellan, and the PDP Research Group.\n",
        "Chapter 1 expresses hope similar to Rosenblatt's vision.\n",
        "\n",
        "-1980s Models and Theoretical Challenges: Many '80s models included a second layer of neurons, addressing issues identified by Minsky and Papert.\n",
        "Neural networks widely used for practical projects during the '80s and '90s.\n",
        "Theoretical misunderstandings hindered progress despite the potential of added layers.\n",
        "\n",
        "- Fulfillment of Neural Network Potential\n",
        "\n",
        "Neural networks now live up to their potential with more layers, improved hardware, data availability, and algorithmic enhancements.\n",
        "Modern neural networks align with Rosenblatt's vision of machines perceiving, recognizing, and identifying surroundings without human training.\n"
      ],
      "metadata": {
        "id": "oJlVLZ4s4I0E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Softwares Used:**\n",
        "\n",
        "1. PyTorch: Considered the most flexible and expressive library for deep learning. Balances speed and simplicity effectively.\n",
        "2. fastai:Most popular for adding higher-level functionality on top of PyTorch. Suited for its layered software architecture.\n",
        "3. Jupyter: Jupyter is the most popular programming experimentation platform.\n",
        "Promotes training models, experimenting with code, and inspecting every stage of the data pre-processing and model development pipeline. It is powerful, flexible, and easy to use. It allows you to include formatted text, code, images, videos, and much more, all within a single interactive document.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ujxbnlA6Ey-o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What Is Machine Learning?**\n",
        "\n",
        "Machine learning represents a paradigm shift in computer programming, differing from traditional approaches by enabling computers to learn and improve from experience rather than following explicitly programmed instructions. Within this framework, deep learning models, powered by neural networks, have recently emerged as powerful tools. The roots of machine learning trace back to Arthur Samuel's 1949 proposal, emphasizing the significance of weight assignments that influence a model's behavior. The training process involves exposing the model to examples, evaluating its performance, and iteratively adjusting weights to enhance its capabilities. This continuous training loop automates the learning process, ultimately yielding a model that, post-training, functions similarly to a conventional computer program, taking inputs and producing desired results. Overall, machine learning broadens the scope of what computers can achieve by leveraging the learning capacity inherent in data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aGr6bcoBvumB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What Is a Neural Network?**\n",
        "\n",
        "Machine learning transforms programming by allowing machines to learn from examples rather than explicit instructions. Neural networks, flexible and empowered by the universal approximation theorem, serve as versatile models that adapt to different problems based on variable weights. Stochastic gradient descent emerges as a universal mechanism for automating weight updates, enhancing model performance. This automated learning framework, applied to tasks like image classification, embodies Arthur Samuel's concept of machines learning from experience, evaluated by accuracy in predicting correct answers."
      ],
      "metadata": {
        "id": "YbOhQGwrzIVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Limitations To Machine Learning**\n",
        "\n",
        "1.   Data Dependence: Models require data to be created and learn patterns from the training data.\n",
        "2. Pattern Learning: Learning involves replicating patterns observed in the input data.\n",
        "3. Label Necessity: Labeled data is essential; examples alone are insufficient for effective training.\n",
        "4. Labeling Challenges: Organizations often lack labeled data, impacting model capabilities.\n",
        "5. Prediction Emphasis: Machine learning models focus on generating predictions rather than prescribing actions.\n",
        "6. Feedback Loops: Models interacting with the environment can create positive feedback loops, reinforcing biases.\n",
        "7. Commercial Impact: Feedback loops may lead to biased recommendations in commercial settings, affecting user behavior.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PMmPzBDS0Nmr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Common Templates for Machine Learning Problems**\n",
        "\n",
        "1. Classification:\n",
        "\n",
        "Task: Assigning a label or category to input data based on its characteristics.\n",
        "Example: Spam email detection, image recognition (identifying objects in images)\n",
        "\n",
        "2. Regression:\n",
        "\n",
        "Task: Predicting a continuous numerical value based on input features.\n",
        "Example: Predicting house prices based on features like size, location, and number of bedrooms.\n",
        "\n",
        "3. Reinforcement Learning:\n",
        "\n",
        "Task: Training an agent to make sequences of decisions by interacting with an environment and receiving feedback in the form of rewards.\n",
        "Example: Training a computer program to play and improve at a game through trial and error.\n",
        "\n",
        "4. Clustering:\n",
        "\n",
        "Task: Grouping similar data points together based on their features, with the goal of discovering inherent structures in the data.\n",
        "Example: Customer segmentation based on purchasing behavior, grouping documents with similar topics.\n",
        "\n",
        "5. Dimension Reduction:\n",
        "\n",
        "Technique: Reducing the number of input features while retaining important information.\n",
        "Purpose: Simplifying models, speeding up training, and avoiding the \"curse of dimensionality.\"\n",
        "\n",
        "6. Generative Learning:\n",
        "\n",
        "Task: Creating new data points that resemble the training data, often used in creating new samples or enhancing datasets.\n",
        "Example: Generating realistic images, creating artificial samples for training purposes.\n"
      ],
      "metadata": {
        "id": "A7qmR65Q6iuc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How Image Recognizer work?**\n",
        "\n",
        "\n",
        "Below code outlines the process of creating an image recognition model using the fastai library.\n",
        "\n",
        "1. Library Import:\n",
        "\n",
        "from fastai.vision.all import *\n",
        "\n",
        "Imports the necessary functions and classes from the fastai.vision library for creating computer vision models.\n",
        "\n",
        "2. Dataset Setup:\n",
        "\n",
        "path = untar_data(URLs.PETS)/'images'\n",
        "\n",
        "Downloads and extracts a standard dataset from the fast.ai collection. Defines the path to the dataset.\n",
        "\n",
        "3. Labeling Function:\n",
        "\n",
        "def is_cat(x): return x[0].isupper()\n",
        "\n",
        "Defines a function (is_cat) to label images based on the filename rule provided by the dataset creators.\n",
        "\n",
        "4. DataLoaders Creation:\n",
        "\n",
        "dls = ImageDataLoaders.from_name_func(\n",
        "    path, get_image_files(path), valid_pct=0.2, seed=42,\n",
        "    label_func=is_cat, item_tfms=Resize(224))\n",
        "\n",
        "Creates DataLoaders using the dataset path, image files, and labeling function. Also defines transformations, validation set percentage, and random seed.\n",
        "\n",
        "5. Model Initialization:\n",
        "\n",
        "learn = vision_learner(dls, resnet34, metrics=error_rate)\n",
        "\n",
        "Initializes a convolutional neural network (CNN) using the resnet34 architecture. Uses error_rate as the evaluation metric.\n",
        "\n",
        "6. Transfer Learning and Fine-tuning:\n",
        "\n",
        "learn.fine_tune(1)\n",
        "\n",
        "Applies transfer learning by fine-tuning a pretrained model. Trains the model for one epoch to adapt to the new dataset, then fine-tunes the entire model for the specified number of epochs.\n",
        "\n",
        "The model architecture, pretrained weights, and fine-tuning allow for efficient learning, and the process involves creating a CNN, adapting it to a specific dataset, and training it to recognize images.\n",
        "\n",
        "With Image Recognizer - visualize the neural network weights learned in each layer of a model.\n",
        "\n",
        "\n",
        "Image recognizers are versatile—they can go beyond images. You can use them for sounds, time series data, even mouse behavior for fraud detection."
      ],
      "metadata": {
        "id": "8WZLP-yJ03xG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Issues in machine learning**\n",
        "\n",
        "1. Overfitting: Overfitting occurs when a machine learning model learns the training data too well, capturing noise or random fluctuations rather than the underlying patterns. As a result, the model performs well on the training data but fails to generalize effectively to new, unseen data.\n",
        "\n",
        "2. High Dimensionality: High dimensionality refers to datasets with a large number of features or dimensions. In such datasets, each data point is described by many variables. High dimensionality can lead to computational challenges, increased risk of overfitting, and difficulties in visualizing and interpreting the data."
      ],
      "metadata": {
        "id": "3N2M3n4D78EO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Validation Sets and Test Sets**\n",
        "\n",
        "To ensure accurate model evaluation, data is divided into a training set and a validation set to prevent overfitting. However, human choices in model exploration pose a risk of overfitting the validation set. To address this, a test set is introduced, providing an additional layer of hidden data for unbiased model evaluation. This approach ensures discipline in the modeling process, combating both automatic memorization tendencies and human biases. Understanding and implementing test and validation sets are crucial for success in AI.\n",
        "\n",
        "To define effective validation and test sets, consider the specific characteristics of your data and future scenarios. In time series data, use the latest dates for validation to simulate real-world usage. For competitions like predicting sales or identifying distracted drivers, ensure the test set represents future conditions by excluding it from training data. Similarly, in tasks like fisheries monitoring, include boats in the validation set that are absent from the training set. Tailor your set definitions to reflect potential differences in future data, enhancing the model's generalization capability.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T8KD2tUw5DBb"
      }
    }
  ]
}